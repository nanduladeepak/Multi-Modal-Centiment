{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 0 datapoints have been dropped.\n",
      "Dataset split\n",
      "Train Set: 16326\n",
      "Validation Set: 1871\n",
      "Test Set: 4659\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel,BertTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "DATA_PATH = '../data/MOSEI/'\n",
    "\n",
    "def to_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_length(x):\n",
    "    return x.shape[1]-(np.sum(x, axis=-1) == 0).sum(1)\n",
    "\n",
    "# first we align to words with averaging, collapse_function receives a list of functions\n",
    "# dataset.align(text_field, collapse_functions=[avg])\n",
    "# load pickle file for unaligned acoustic and visual source\n",
    "pickle_filename = DATA_PATH+'mosei_senti_data_noalign.pkl'\n",
    "csv_filename = DATA_PATH+'MOSEI-label.csv'\n",
    "\n",
    "with open(pickle_filename, 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "\n",
    "# read csv file for label and text\n",
    "df = pd.read_csv(csv_filename)\n",
    "text = df['text']\n",
    "vid = df['video_id']\n",
    "cid = df['clip_id']\n",
    "\n",
    "train_split_noalign = d['train']\n",
    "dev_split_noalign = d['valid']\n",
    "test_split_noalign = d['test']\n",
    "\n",
    "# a sentinel epsilon for safe division, without it we will replace illegal values with a constant\n",
    "EPS = 1e-6\n",
    "\n",
    "# place holders for the final train/dev/test dataset\n",
    "train = train = []\n",
    "dev = dev = []\n",
    "test = test = []\n",
    "\n",
    "# define a regular expression to extract the video ID out of the keys\n",
    "# pattern = re.compile('(.*)\\[.*\\]')\n",
    "pattern = re.compile('(.*)_([.*])')\n",
    "num_drop = 0 # a counter to count how many data points went into some processing issues\n",
    "\n",
    "v = np.concatenate((train_split_noalign['vision'],dev_split_noalign['vision'], test_split_noalign['vision']),axis=0)\n",
    "vlens = get_length(v)\n",
    "\n",
    "a = np.concatenate((train_split_noalign['audio'],dev_split_noalign['audio'], test_split_noalign['audio']),axis=0)\n",
    "alens = get_length(a)\n",
    "\n",
    "label = np.concatenate((train_split_noalign['labels'],dev_split_noalign['labels'], test_split_noalign['labels']),axis=0)\n",
    "\n",
    "L_V = v.shape[1]\n",
    "L_A = a.shape[1]\n",
    "\n",
    "\n",
    "all_id = np.concatenate((train_split_noalign['id'], dev_split_noalign['id'], test_split_noalign['id']),axis=0)[:,0]\n",
    "all_id_list = all_id.tolist()\n",
    "\n",
    "train_size = len(train_split_noalign['id'])\n",
    "dev_size = len(dev_split_noalign['id'])\n",
    "test_size = len(test_split_noalign['id'])\n",
    "\n",
    "dev_start = train_size\n",
    "test_start = train_size + dev_size\n",
    "\n",
    "all_csv_id = [(vid[i], str(cid[i])) for i in range(len(vid))]\n",
    "\n",
    "for i, idd in enumerate(all_id_list):\n",
    "    # get the video ID and the features out of the aligned dataset\n",
    "\n",
    "    # matching process\n",
    "    try:\n",
    "        index = i\n",
    "    except:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "\n",
    "    _words = text[index].split()\n",
    "    _label = label[i].astype(np.float32)\n",
    "    _visual = v[i]\n",
    "    _acoustic = a[i]\n",
    "    _vlen = vlens[i]\n",
    "    _alen = alens[i]\n",
    "    _id = '{}[{}]'.format(all_csv_id[0], all_csv_id[1])           \n",
    "\n",
    "    # remove nan values\n",
    "    # label = np.nan_to_num(label)\n",
    "    _visual = np.nan_to_num(_visual)\n",
    "    _acoustic = np.nan_to_num(_acoustic)\n",
    "\n",
    "    # remove speech pause tokens - this is in general helpful\n",
    "    # we should remove speech pauses and corresponding visual/acoustic features together\n",
    "    # otherwise modalities would no longer be aligned\n",
    "    actual_words = []\n",
    "    words = []\n",
    "    visual = []\n",
    "    acoustic = []\n",
    "\n",
    "    for word in _words:\n",
    "        actual_words.append(word)\n",
    "\n",
    "    visual = _visual[L_V - _vlen:,:]\n",
    "    acoustic = _acoustic[L_A - _alen:,:]\n",
    "\n",
    "    if i < dev_start:\n",
    "        train.append((words, visual, acoustic, actual_words, _vlen, _alen, _label, idd))\n",
    "    elif i >= dev_start and i < test_start:\n",
    "        dev.append((words, visual, acoustic, actual_words, _vlen, _alen, _label, idd))\n",
    "    elif i >= test_start:\n",
    "        test.append((words, visual, acoustic, actual_words, _vlen, _alen, _label, idd))\n",
    "    else:\n",
    "        print(f\"Found video that doesn't belong to any splits: {idd}\")\n",
    "\n",
    "\n",
    "# print(f\"Total number of {num_drop} datapoints have been dropped.\")\n",
    "print(f\"Total number of {num_drop} datapoints have been dropped.\")\n",
    "print(\"Dataset split\")\n",
    "print(\"Train Set: {}\".format(len(train)))\n",
    "print(\"Validation Set: {}\".format(len(dev)))\n",
    "print(\"Test Set: {}\".format(len(test)))\n",
    "\n",
    "# Save glove embeddings cache too\n",
    "# self.pretrained_emb = pretrained_emb = load_emb(word2id, config.word_emb_path)\n",
    "# torch.save((pretrained_emb, word2id), CACHE_PATH)\n",
    "pretrained_emb = None\n",
    "\n",
    "# Save pickles\n",
    "to_pickle(train, DATA_PATH + '/dftrain.pkl')\n",
    "to_pickle(dev, DATA_PATH + '/dfdev.pkl')\n",
    "to_pickle(test, DATA_PATH + '/dftest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train,columns=['words', 'visual', 'acoustic', 'actual_words', '_vlen', '_alen', '_label', 'idd'])\n",
    "dev_df = pd.DataFrame(dev,columns=['words', 'visual', 'acoustic', 'actual_words', '_vlen', '_alen', '_label', 'idd'])\n",
    "test_df = pd.DataFrame(test,columns=['words', 'visual', 'acoustic', 'actual_words', '_vlen', '_alen', '_label', 'idd'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPadding(df, colName, dim):\n",
    "    max_length_x = df[colName].apply(lambda x: x.shape[0] if isinstance(x, np.ndarray) else 0).max()\n",
    "\n",
    "    # Perform padding within the DataFrame\n",
    "    df[colName] = df[colName].apply(lambda x: np.vstack([x, np.zeros((max_length_x - x.shape[0], dim))]) if isinstance(x, np.ndarray) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "addPadding(train_df,'visual',35)\n",
    "addPadding(train_df,'acoustic',74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 35)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_df['visual'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 74)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_df['acoustic'][74])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_x = train_df['actual_words'].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_acoustic_model(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "def create_video_model(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    # Add a channel dimension to the input data\n",
    "    x = tf.keras.layers.Reshape((*input_shape, 1))(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    outputs = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def create_bert_model(max_length):\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "    cls_token = bert_output[:, 0, :]\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=cls_token)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_model(acoustic_input_shape, visual_input_shape, text_max_length):\n",
    "    acoustic_model = create_acoustic_model(acoustic_input_shape)\n",
    "    bert_model = create_bert_model(text_max_length)\n",
    "    video_model = create_video_model(visual_input_shape)\n",
    "\n",
    "\n",
    "    # Combine the models\n",
    "    combined_input = tf.keras.layers.Concatenate()([acoustic_model.output, bert_model.output, video_model.output])\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(combined_input)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[acoustic_model.input, bert_model.input, video_model.input], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Key, is, part, of, the, people, that, we, use...\n",
       "1        [They've, been, able, to, find, solutions, or,...\n",
       "2        [We're, a, huge, user, of, adhesives, for, our...\n",
       "3        [Key, Polymer, brings, a, technical, aspect, t...\n",
       "4        [Key, brings, those, types, of, aspects, to, a...\n",
       "                               ...                        \n",
       "16321    [I, read, other, articles,, what, other, train...\n",
       "16322                               [I, do, all, of, that]\n",
       "16323    [Now,, if, this, sounds, like, something, you'...\n",
       "16324    [I, actually, speak, to, the, experts, myself,...\n",
       "16325    [And, we’ve, seen, some, programs,, we’ve, see...\n",
       "Name: actual_words, Length: 16326, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['actual_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_combined_model(acoustic_input_shape=(74,500), visual_input_shape=(35,500), text_max_length=307)\n",
    "\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Encode text data using BERT tokenizer\n",
    "# @tf.function\n",
    "# def encode_text_data(text_data, max_length=307):\n",
    "#     encoded_texts = bert_tokenizer(text_data, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "#     return encoded_texts\n",
    "\n",
    "# # Preprocess text data\n",
    "# train_df['encode_text_data'] = train_df['actual_words'].apply(encode_text_data)\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit([train_df['acoustic'], train_df['encode_text_data'], train_df['visual']], train_df['_label'], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_combined_model(acoustic_input_shape=(74,500), visual_input_shape=(35,500), text_max_length=307)\n",
    "\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Encode text data using BERT tokenizer\n",
    "# def encode_text_data(text_data, max_length=307):\n",
    "#     encoded_texts = bert_tokenizer(text_data, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "#     return encoded_texts\n",
    "\n",
    "# # Preprocess text data\n",
    "# train_df['encode_text_data'] = train_df['actual_words'].apply(encode_text_data)\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit([train_df['acoustic'], train_df['encode_text_data'], train_df['visual']], train_df['_label'], epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [[1.0]]\n",
       "1        [[0.6666667]]\n",
       "2              [[0.0]]\n",
       "3              [[0.0]]\n",
       "4              [[1.0]]\n",
       "             ...      \n",
       "16321          [[0.0]]\n",
       "16322          [[0.0]]\n",
       "16323    [[0.6666667]]\n",
       "16324          [[1.0]]\n",
       "16325          [[1.0]]\n",
       "Name: _label, Length: 16326, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 74, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 74, 500), dtype=tf.float32, name='input_91'), name='input_91', description=\"created by layer 'input_91'\"), but it was called on an input with incompatible shape (None, 500, 74).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 500), dtype=tf.int32, name='input_92'), name='input_92', description=\"created by layer 'input_92'\"), but it was called on an input with incompatible shape (None, 1, 500).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 500), dtype=tf.int32, name='input_93'), name='input_93', description=\"created by layer 'input_93'\"), but it was called on an input with incompatible shape (None, 1, 500).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 500), dtype=tf.int32, name='input_94'), name='input_94', description=\"created by layer 'input_94'\"), but it was called on an input with incompatible shape (None, 1, 500).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 35, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 35, 500), dtype=tf.float32, name='input_95'), name='input_95', description=\"created by layer 'input_95'\"), but it was called on an input with incompatible shape (None, 500, 35).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_filejqo70sky.py\", line 30, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"tf_bert_model_18\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n            outputs = self.bert(\n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py\", line 75, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer \"bert\" (type TFBertMainLayer):\n          • self=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • input_ids=None\n          • attention_mask=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • token_type_ids=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=True\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer \"tf_bert_model_18\" (type TFBertModel):\n      • self=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • input_ids=None\n      • attention_mask=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • token_type_ids=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • encoder_hidden_states=None\n      • encoder_attention_mask=None\n      • past_key_values=None\n      • use_cache=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m# Train the model using the prepared data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 37\u001b[0m model\u001b[39m.\u001b[39;49mfit([acoustic_data, input_ids_data, attention_mask_data, token_type_ids_data, visual_data], label_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_filezhbqqat2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_filejqo70sky.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     29\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mbert, (), \u001b[39mdict\u001b[39m(input_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(input_ids), attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(attention_mask), token_type_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(token_type_ids), position_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(position_ids), head_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(head_mask), inputs_embeds\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(inputs_embeds), encoder_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_hidden_states), encoder_attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_attention_mask), past_key_values\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(past_key_values), use_cache\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(use_cache), output_attentions\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_hidden_states), return_dict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(return_dict), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py:75\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     73\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(input_ids) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(inputs_embeds) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m (batch_size, seq_length) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_shape)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_filejqo70sky.py\", line 30, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"tf_bert_model_18\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n            outputs = self.bert(\n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py\", line 75, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer \"bert\" (type TFBertMainLayer):\n          • self=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • input_ids=None\n          • attention_mask=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • token_type_ids=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=True\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer \"tf_bert_model_18\" (type TFBertModel):\n      • self=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • input_ids=None\n      • attention_mask=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • token_type_ids=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • encoder_hidden_states=None\n      • encoder_attention_mask=None\n      • past_key_values=None\n      • use_cache=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# model = create_combined_model(acoustic_input_shape=(74, 500), visual_input_shape=(35, 500), text_max_length=500)\n",
    "\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Encode text data using BERT tokenizer\n",
    "# def encode_text_data(text_data, max_length=307):\n",
    "#     # Join the list of words into a single string\n",
    "#     text = ' '.join(text_data)\n",
    "#     encoded_texts = bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "#     # Return the required BERT input keys as separate tensors\n",
    "#     return (\n",
    "#         tf.convert_to_tensor(encoded_texts['input_ids']),\n",
    "#         tf.convert_to_tensor(encoded_texts['attention_mask']),\n",
    "#         tf.convert_to_tensor(encoded_texts['token_type_ids']),\n",
    "#     )\n",
    "\n",
    "# # Preprocess text data and create a tf.data.Dataset\n",
    "# text_data = train_df['actual_words'].apply(encode_text_data, max_length=500)\n",
    "\n",
    "# # Prepare the rest of the data\n",
    "# acoustic_data = tf.convert_to_tensor(np.stack(train_df['acoustic']))\n",
    "# visual_data = tf.convert_to_tensor(np.stack(train_df['visual']))\n",
    "# # label_data = tf.convert_to_tensor(np.array(train_df['_label']))\n",
    "# label_data = np.array([int(label[0][0]) for label in train_df['_label']])\n",
    "\n",
    "# input_ids_list, attention_mask_list, token_type_ids_list = zip(*text_data)\n",
    "\n",
    "# # Convert the lists of tensors to numpy arrays\n",
    "# input_ids_data = np.array(input_ids_list)\n",
    "# attention_mask_data = np.array(attention_mask_list)\n",
    "# token_type_ids_data = np.array(token_type_ids_list)\n",
    "\n",
    "# # Train the model using the prepared data\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit([acoustic_data, input_ids_data, attention_mask_data, token_type_ids_data, visual_data], label_data, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/ipykernel_27157/2069454755.py:31: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  label_data = np.array([int(label[0]) for label in train_df['_label']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 74, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 74, 500), dtype=tf.float32, name='input_96'), name='input_96', description=\"created by layer 'input_96'\"), but it was called on an input with incompatible shape (None, 500, 74).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 500), dtype=tf.int32, name='input_97'), name='input_97', description=\"created by layer 'input_97'\"), but it was called on an input with incompatible shape (None, 1, 500).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 500), dtype=tf.int32, name='input_98'), name='input_98', description=\"created by layer 'input_98'\"), but it was called on an input with incompatible shape (None, 1, 500).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 500), dtype=tf.int32, name='input_99'), name='input_99', description=\"created by layer 'input_99'\"), but it was called on an input with incompatible shape (None, 1, 500).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 35, 500) for input KerasTensor(type_spec=TensorSpec(shape=(None, 35, 500), dtype=tf.float32, name='input_100'), name='input_100', description=\"created by layer 'input_100'\"), but it was called on an input with incompatible shape (None, 500, 35).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_filejqo70sky.py\", line 30, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"tf_bert_model_19\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n            outputs = self.bert(\n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py\", line 75, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer \"bert\" (type TFBertMainLayer):\n          • self=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • input_ids=None\n          • attention_mask=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • token_type_ids=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=True\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer \"tf_bert_model_19\" (type TFBertModel):\n      • self=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • input_ids=None\n      • attention_mask=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • token_type_ids=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • encoder_hidden_states=None\n      • encoder_attention_mask=None\n      • past_key_values=None\n      • use_cache=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m# Train the model using the prepared data\u001b[39;00m\n\u001b[1;32m     41\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 42\u001b[0m model\u001b[39m.\u001b[39;49mfit([acoustic_data, input_ids_data, attention_mask_data, token_type_ids_data, visual_data], label_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_filezhbqqat2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_filejqo70sky.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     29\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mbert, (), \u001b[39mdict\u001b[39m(input_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(input_ids), attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(attention_mask), token_type_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(token_type_ids), position_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(position_ids), head_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(head_mask), inputs_embeds\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(inputs_embeds), encoder_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_hidden_states), encoder_attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_attention_mask), past_key_values\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(past_key_values), use_cache\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(use_cache), output_attentions\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_hidden_states), return_dict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(return_dict), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py:75\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     73\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(input_ids) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(inputs_embeds) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m (batch_size, seq_length) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_shape)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_filejqo70sky.py\", line 30, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"tf_bert_model_19\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n            outputs = self.bert(\n        File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file0vg8_j1w.py\", line 36, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/folders/zj/hd8_xvbd3fvbp9wj5412cw_c0000gn/T/__autograph_generated_file8oc0f77c.py\", line 75, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer \"bert\" (type TFBertMainLayer):\n          • self=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • input_ids=None\n          • attention_mask=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • token_type_ids=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=True\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer \"tf_bert_model_19\" (type TFBertModel):\n      • self=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • input_ids=None\n      • attention_mask=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • token_type_ids=tf.Tensor(shape=(None, 1, 500), dtype=int32)\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • encoder_hidden_states=None\n      • encoder_attention_mask=None\n      • past_key_values=None\n      • use_cache=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# ... (Previous code before model creation remains unchanged)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model = create_combined_model(acoustic_input_shape=(74, 500), visual_input_shape=(35, 500), text_max_length=500)\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode text data using BERT tokenizer\n",
    "def encode_text_data(text_data, max_length=307):\n",
    "    # Join the list of words into a single string\n",
    "    text = ' '.join(text_data)\n",
    "    encoded_texts = bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "    # Return the required BERT input keys as separate tensors\n",
    "    return (\n",
    "        tf.convert_to_tensor(encoded_texts['input_ids']),\n",
    "        tf.convert_to_tensor(encoded_texts['attention_mask']),\n",
    "        tf.convert_to_tensor(encoded_texts['token_type_ids']),\n",
    "    )\n",
    "\n",
    "# Filter out rows with missing 'actual_words'\n",
    "train_df = train_df[train_df['actual_words'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Preprocess text data and create a tf.data.Dataset\n",
    "text_data = train_df['actual_words'].apply(encode_text_data, max_length=500)\n",
    "\n",
    "# Prepare the rest of the data\n",
    "acoustic_data = np.stack(train_df['acoustic'])\n",
    "visual_data = np.stack(train_df['visual'])\n",
    "label_data = np.array([int(label[0]) for label in train_df['_label']])\n",
    "\n",
    "input_ids_list, attention_mask_list, token_type_ids_list = zip(*text_data)\n",
    "\n",
    "# Convert the lists of tensors to numpy arrays\n",
    "input_ids_data = np.array(input_ids_list)\n",
    "attention_mask_data = np.array(attention_mask_list)\n",
    "token_type_ids_data = np.array(token_type_ids_list)\n",
    "\n",
    "\n",
    "# Train the model using the prepared data\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit([acoustic_data, input_ids_data, attention_mask_data, token_type_ids_data, visual_data], label_data, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "def create_acoustic_model(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "def create_video_model(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    outputs = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def create_bert_model(max_length):\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model({\"input_ids\": input_ids, \"attention_mask\": attention_mask})[1]\n",
    "    cls_token = bert_output[0]\n",
    "\n",
    "    model = tf.keras.Model(inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask}, outputs=cls_token)\n",
    "    return model\n",
    "    \n",
    "def create_combined_model(acoustic_input_shape, visual_input_shape, text_max_length):\n",
    "    acoustic_model = create_acoustic_model(acoustic_input_shape)\n",
    "    bert_model = create_bert_model(text_max_length)\n",
    "    video_model = create_video_model(visual_input_shape)\n",
    "\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    acoustic_input = tf.keras.layers.Input(shape=acoustic_input_shape)\n",
    "    visual_input = tf.keras.layers.Input(shape=visual_input_shape)\n",
    "\n",
    "    # Assuming the output shape of your bert_model is (batch_size, bert_output_dim)\n",
    "    bert_output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "    # Assuming the output shape of your video_model is (batch_size, video_output_dim)\n",
    "    visual_output = video_model(visual_input)\n",
    "\n",
    "    # Assuming the output shape of your acoustic_model is (batch_size, acoustic_output_dim)\n",
    "    acoustic_output = acoustic_model(acoustic_input)\n",
    "\n",
    "    # Modify the next lines according to your actual requirements for combining the models\n",
    "    combined_input = tf.keras.layers.Concatenate()([acoustic_output, bert_output, visual_output])\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(combined_input)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[acoustic_input, {'input_ids': input_ids, 'attention_mask': attention_mask}, visual_input], outputs=outputs)\n",
    "    return model\n",
    "# Create the model\n",
    "\n",
    "# Compile the model for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_model(acoustic_input_shape, visual_input_shape, text_max_length):\n",
    "    acoustic_model = create_acoustic_model(acoustic_input_shape)\n",
    "    bert_model = create_bert_model(text_max_length)\n",
    "    video_model = create_video_model(visual_input_shape)\n",
    "\n",
    "    acoustic_input = tf.keras.layers.Input(shape=acoustic_input_shape)\n",
    "    bert_input_ids = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    bert_attention_mask = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    bert_token_type_ids = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    visual_input = tf.keras.layers.Input(shape=visual_input_shape)\n",
    "\n",
    "    # Assuming the output shape of your acoustic_model is (batch_size, acoustic_output_dim)\n",
    "    acoustic_output = acoustic_model(acoustic_input)\n",
    "\n",
    "    # Assuming the output shape of your bert_model is (batch_size, bert_output_dim)\n",
    "    bert_output = bert_model([bert_input_ids, bert_attention_mask, bert_token_type_ids])\n",
    "\n",
    "    # Assuming the output shape of your video_model is (batch_size, video_output_dim)\n",
    "    visual_output = video_model(visual_input)\n",
    "\n",
    "    # Combine the outputs from the individual models\n",
    "    combined_input = tf.keras.layers.Concatenate()([acoustic_output, bert_output, visual_output])\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(combined_input)\n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(x)  # Using 'linear' activation for regression\n",
    "\n",
    "    model = tf.keras.Model(inputs=[acoustic_input, bert_input_ids, bert_attention_mask, bert_token_type_ids, visual_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "def create_combined_model(acoustic_input_shape, visual_input_shape, text_max_length):\n",
    "    acoustic_model = create_acoustic_model(acoustic_input_shape)\n",
    "    bert_model = create_bert_model(text_max_length)\n",
    "    video_model = create_video_model(visual_input_shape)\n",
    "\n",
    "    acoustic_input = tf.keras.layers.Input(shape=acoustic_input_shape)\n",
    "    bert_input_ids = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    bert_attention_mask = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    bert_token_type_ids = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    visual_input = tf.keras.layers.Input(shape=visual_input_shape)\n",
    "\n",
    "    acoustic_output = acoustic_model(acoustic_input)\n",
    "    bert_output = bert_model([bert_input_ids, bert_attention_mask, bert_token_type_ids])\n",
    "    visual_output = video_model(visual_input)\n",
    "\n",
    "    # Combine the outputs from the individual models\n",
    "    combined_input = tf.keras.layers.Concatenate()([acoustic_output, bert_output, visual_output])\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(combined_input)\n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(x)  # Using 'linear' activation for regression\n",
    "\n",
    "    model = tf.keras.Model(inputs=[acoustic_input, bert_input_ids, bert_attention_mask, bert_token_type_ids, visual_input], outputs=output)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_df['words'] = train_df['actual_words'].apply(processText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [[1.0]]\n",
       "1        [[0.6666667]]\n",
       "2              [[0.0]]\n",
       "3              [[0.0]]\n",
       "4              [[1.0]]\n",
       "             ...      \n",
       "16321          [[0.0]]\n",
       "16322          [[0.0]]\n",
       "16323    [[0.6666667]]\n",
       "16324          [[1.0]]\n",
       "16325          [[1.0]]\n",
       "Name: _label, Length: 16326, dtype: object"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 128), (768,), (None, 32)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_categorical\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m create_combined_model(acoustic_input_shape\u001b[39m=\u001b[39;49m(\u001b[39m74\u001b[39;49m, \u001b[39m500\u001b[39;49m), visual_input_shape\u001b[39m=\u001b[39;49m(\u001b[39m35\u001b[39;49m, \u001b[39m500\u001b[39;49m), text_max_length\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Encode text data using BERT tokenizer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_text_data\u001b[39m(text_data, max_length\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Load the BERT tokenizer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[198], line 55\u001b[0m, in \u001b[0;36mcreate_combined_model\u001b[0;34m(acoustic_input_shape, visual_input_shape, text_max_length)\u001b[0m\n\u001b[1;32m     52\u001b[0m acoustic_output \u001b[39m=\u001b[39m acoustic_model(acoustic_input)\n\u001b[1;32m     54\u001b[0m \u001b[39m# Modify the next lines according to your actual requirements for combining the models\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m combined_input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mConcatenate()([acoustic_output, bert_output, visual_output])\n\u001b[1;32m     56\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m128\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(combined_input)\n\u001b[1;32m     57\u001b[0m outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m)(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/keras/layers/merging/concatenate.py:114\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    112\u001b[0m ranks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mlen\u001b[39m(shape) \u001b[39mfor\u001b[39;00m shape \u001b[39min\u001b[39;00m shape_set)\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ranks) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err_msg)\n\u001b[1;32m    115\u001b[0m \u001b[39m# Get the only rank for the set.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m (rank,) \u001b[39m=\u001b[39m ranks\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 128), (768,), (None, 32)]"
     ]
    }
   ],
   "source": [
    "# ... (Previous code before model creation remains unchanged)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model = create_combined_model(acoustic_input_shape=(74, 500), visual_input_shape=(35, 500), text_max_length=500)\n",
    "\n",
    "# Encode text data using BERT tokenizer\n",
    "def encode_text_data(text_data, max_length=500):\n",
    "    # Load the BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Define the maximum sequence length for padding/truncating\n",
    "    max_length = 512\n",
    "\n",
    "    # Tokenize the list of strings and convert them to input IDs, attention masks, and token type IDs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in text_data:\n",
    "        # Tokenize the text and add the special [CLS] and [SEP] tokens\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Text to encode\n",
    "                            add_special_tokens = True, # Add [CLS] and [SEP] tokens\n",
    "                            max_length = max_length,   # Pad/truncate to a maximum length\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Generate attention masks\n",
    "                            return_token_type_ids = False,   # Do not generate token type IDs\n",
    "                            truncation=True,\n",
    "                            )\n",
    "        \n",
    "        # Add the encoded sequence and attention mask to the lists\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists to tensors\n",
    "    input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "    \n",
    "    # Return a tuple of input IDs and attention masks\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Filter out rows with missing 'actual_words'\n",
    "# train_df = train_df[train_df['actual_words'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess text data and create a tf.data.Dataset\n",
    "text_data = encode_text_data(train_df['words'].tolist())\n",
    "\n",
    "\n",
    "# Prepare the rest of the data\n",
    "acoustic_data = np.stack(train_df['acoustic'])\n",
    "visual_data = np.stack(train_df['visual'])\n",
    "label_data = np.array([label[0] for label in train_df['_label']])\n",
    "\n",
    "input_ids_list, attention_mask_list, token_type_ids_list = zip(*text_data)\n",
    "\n",
    "# Convert the lists of tensors to numpy arrays\n",
    "input_ids_data = np.array(input_ids_list)\n",
    "attention_mask_data = np.array(attention_mask_list)\n",
    "token_type_ids_data = np.array(token_type_ids_list)\n",
    "\n",
    "# Convert the label_data to float32 as it is expected for regression\n",
    "label_data = label_data.astype(np.float32)\n",
    "\n",
    "# Train the model using the prepared data for regression\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "# acoustic_data = acoustic_data.transpose(0, 2, 1)\n",
    "model.fit(\n",
    "    [acoustic_data, {'input_ids': text_data[0], 'attention_mask': text_data[1]}, visual_data],\n",
    "    label_data,\n",
    "    epochs=10\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('env_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7f7a6c7a328c331394eefecd3ffebe18a341a9e21c7b3790bdb1b050ca99952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
