{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16326,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 174\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m    171\u001b[0m \u001b[39m# ... Rest of the model code ...\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[1;32m    173\u001b[0m \u001b[39m# Preprocessing\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m acoustic_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray([item[\u001b[39m2\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m train])\n\u001b[1;32m    175\u001b[0m video_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([item[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m train])\n\u001b[1;32m    176\u001b[0m text_data \u001b[39m=\u001b[39m [item[\u001b[39m3\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m train]\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16326,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel,BertTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '../data/MOSEI/'\n",
    "\n",
    "def to_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_length(x):\n",
    "    return x.shape[1] - (np.sum(x, axis=-1) == 0).sum(1)\n",
    "\n",
    "# Load pickle file for unaligned acoustic and visual source\n",
    "pickle_filename = DATA_PATH + 'mosei_senti_data_noalign.pkl'\n",
    "csv_filename = DATA_PATH + 'MOSEI-label.csv'\n",
    "\n",
    "with open(pickle_filename, 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "\n",
    "# Read csv file for label and text\n",
    "df = pd.read_csv(csv_filename)\n",
    "text = df['text']\n",
    "vid = df['video_id']\n",
    "cid = df['clip_id']\n",
    "\n",
    "train_split_noalign = d['train']\n",
    "dev_split_noalign = d['valid']\n",
    "test_split_noalign = d['test']\n",
    "\n",
    "# Define a regular expression to extract the video ID out of the keys\n",
    "# pattern = re.compile('(.*)\\[.*\\]')\n",
    "pattern = re.compile('(.*)_([.*])')\n",
    "num_drop = 0 # a counter to count how many data points went into some processing issues\n",
    "\n",
    "v = np.concatenate((train_split_noalign['vision'], dev_split_noalign['vision'], test_split_noalign['vision']), axis=0)\n",
    "vlens = get_length(v)\n",
    "\n",
    "a = np.concatenate((train_split_noalign['audio'], dev_split_noalign['audio'], test_split_noalign['audio']), axis=0)\n",
    "alens = get_length(a)\n",
    "\n",
    "label = np.concatenate((train_split_noalign['labels'], dev_split_noalign['labels'], test_split_noalign['labels']), axis=0)\n",
    "\n",
    "L_V = v.shape[1]\n",
    "L_A = a.shape[1]\n",
    "\n",
    "all_id = np.concatenate((train_split_noalign['id'], dev_split_noalign['id'], test_split_noalign['id']), axis=0)[:, 0]\n",
    "all_id_list = all_id.tolist()\n",
    "\n",
    "train_size = len(train_split_noalign['id'])\n",
    "dev_size = len(dev_split_noalign['id'])\n",
    "test_size = len(test_split_noalign['id'])\n",
    "\n",
    "dev_start = train_size\n",
    "test_start = train_size + dev_size\n",
    "\n",
    "all_csv_id = [(vid[i], str(cid[i])) for i in range(len(vid))]\n",
    "\n",
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "\n",
    "for i, idd in enumerate(all_id_list):\n",
    "    # get the video ID and the features out of the aligned dataset\n",
    "\n",
    "    # matching process\n",
    "    try:\n",
    "        index = i\n",
    "    except:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "\n",
    "    _words = text[index].split()\n",
    "    _label = label[i].astype(np.float32)\n",
    "    _visual = v[i]\n",
    "    _acoustic = a[i]\n",
    "    _vlen = vlens[i]\n",
    "    _alen = alens[i]\n",
    "    _id = '{}[{}]'.format(all_csv_id[0], all_csv_id[1])\n",
    "\n",
    "    # Remove nan values\n",
    "    _visual = np.nan_to_num(_visual)\n",
    "    _acoustic = np.nan_to_num(_acoustic)\n",
    "\n",
    "    # Remove speech pause tokens - this is in general helpful\n",
    "    # We should remove speech pauses and corresponding visual/acoustic features together\n",
    "    # Otherwise, modalities would no longer be aligned\n",
    "    actual_words = []\n",
    "    words = []\n",
    "    visual = []\n",
    "    acoustic = []\n",
    "\n",
    "    for word in _words:\n",
    "        actual_words.append(word)\n",
    "\n",
    "    visual = _visual[L_V - _vlen:, :]\n",
    "    acoustic = _acoustic[L_A - _alen:, :]\n",
    "\n",
    "    if i < dev_start:\n",
    "        train.append((words, visual, acoustic, actual_words, _vlen, _alen, _label, idd))\n",
    "    elif i >= dev_start and i < test_start:\n",
    "        dev.append((words, visual, acoustic, actual_words, _vlen, _alen, _label, idd))\n",
    "    elif i >= test_start:\n",
    "        test.append((words, visual, acoustic, actual_words, _vlen, _alen, _label, idd))\n",
    "    else:\n",
    "        print(f\"Found video that doesn't belong to any splits: {idd}\")\n",
    "\n",
    "# ... Rest of the data processing code ...\n",
    "train_df = pd.DataFrame(train,columns=['words', 'visual', 'acoustic', 'actual_words', '_vlen', '_alen', '_label', 'idd'])\n",
    "dev_df = pd.DataFrame(dev,columns=['words', 'visual', 'acoustic', 'actual_words', '_vlen', '_alen', '_label', 'idd'])\n",
    "test_df = pd.DataFrame(test,columns=['words', 'visual', 'acoustic', 'actual_words', '_vlen', '_alen', '_label', 'idd'])\n",
    "\n",
    "# Define the model using TensorFlow\n",
    "def create_acoustic_model(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "def create_video_model(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Reshape(input_shape)(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    outputs = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def create_bert_model(max_length):\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "    cls_token = bert_output[:, 0, :]\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=cls_token)\n",
    "    return model\n",
    "\n",
    "def create_combined_model(acoustic_input_shape, visual_input_shape, text_max_length):\n",
    "    acoustic_model = create_acoustic_model(acoustic_input_shape)\n",
    "    bert_model = create_bert_model(text_max_length)\n",
    "    video_model = create_video_model(visual_input_shape)\n",
    "\n",
    "    # ... Create the CNN model for visual input ...\n",
    "\n",
    "    # Combine the models\n",
    "    combined_input = tf.keras.layers.Concatenate()([acoustic_model.output, bert_model.output, video_model.output])\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(combined_input)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[acoustic_model.input, bert_model.input, video_model.input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# ... Rest of the model code ...\n",
    "\n",
    "# Preprocessing\n",
    "acoustic_data = np.array([item[2] for item in train])\n",
    "video_data = np.array([item[1] for item in train])\n",
    "text_data = [item[3] for item in train]\n",
    "labels = np.array([item[6] for item in train])\n",
    "\n",
    "# Normalize the acoustic and visual data\n",
    "acoustic_scaler = StandardScaler()\n",
    "video_scaler = StandardScaler()\n",
    "acoustic_data = acoustic_scaler.fit_transform(acoustic_data)\n",
    "video_data = video_scaler.fit_transform(video_data)\n",
    "\n",
    "# BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode text data using BERT tokenizer\n",
    "def encode_text_data(text_data, max_length=50):\n",
    "    encoded_texts = bert_tokenizer(text_data, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "    return encoded_texts\n",
    "\n",
    "# Preprocess text data\n",
    "text_data_encoded = encode_text_data(text_data)\n",
    "\n",
    "# ... Rest of the training code ...\n",
    "\n",
    "# Create TensorFlow Dataset\n",
    "def create_tf_dataset(acoustic_data, video_data, text_data_encoded, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((acoustic_data, video_data, text_data_encoded, labels))\n",
    "    return dataset\n",
    "\n",
    "# Create a dataset for training\n",
    "batch_size = 32\n",
    "train_dataset = create_tf_dataset(acoustic_data, video_data, text_data_encoded, labels)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(labels)).batch(batch_size)\n",
    "\n",
    "# Define the model\n",
    "acoustic_input_shape = (acoustic_data.shape[1],)\n",
    "visual_input_shape = (video_data.shape[1],)\n",
    "text_max_length = 50  # This should be the maximum length of your text data\n",
    "model = create_combined_model(acoustic_input_shape, visual_input_shape, text_max_length)\n",
    "\n",
    "# ... Rest of the training code ...\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_dataset, epochs=10)\n",
    "\n",
    "# ... Rest of the training code ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('env_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7f7a6c7a328c331394eefecd3ffebe18a341a9e21c7b3790bdb1b050ca99952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
