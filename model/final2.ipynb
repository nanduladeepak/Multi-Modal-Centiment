{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 0 datapoints have been dropped.\n",
      "Dataset split\n",
      "Train Set: 16326\n",
      "Validation Set: 1871\n",
      "Test Set: 4659\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel,BertTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "DATA_PATH = '../data/MOSEI/'\n",
    "\n",
    "def to_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_length(x):\n",
    "    return x.shape[1]-(np.sum(x, axis=-1) == 0).sum(1)\n",
    "\n",
    "# first we align to words with averaging, collapse_function receives a list of functions\n",
    "# dataset.align(text_field, collapse_functions=[avg])\n",
    "# load pickle file for unaligned acoustic and visual source\n",
    "pickle_filename = DATA_PATH+'mosei_senti_data_noalign.pkl'\n",
    "csv_filename = DATA_PATH+'MOSEI-label.csv'\n",
    "\n",
    "with open(pickle_filename, 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "\n",
    "# read csv file for label and text\n",
    "df = pd.read_csv(csv_filename)\n",
    "text = df['text']\n",
    "vid = df['video_id']\n",
    "cid = df['clip_id']\n",
    "\n",
    "train_split_noalign = d['train']\n",
    "dev_split_noalign = d['valid']\n",
    "test_split_noalign = d['test']\n",
    "\n",
    "# a sentinel epsilon for safe division, without it we will replace illegal values with a constant\n",
    "EPS = 1e-6\n",
    "\n",
    "# place holders for the final train/dev/test dataset\n",
    "train = train = []\n",
    "dev = dev = []\n",
    "test = test = []\n",
    "\n",
    "# define a regular expression to extract the video ID out of the keys\n",
    "# pattern = re.compile('(.*)\\[.*\\]')\n",
    "pattern = re.compile('(.*)_([.*])')\n",
    "num_drop = 0 # a counter to count how many data points went into some processing issues\n",
    "\n",
    "v = np.concatenate((train_split_noalign['vision'],dev_split_noalign['vision'], test_split_noalign['vision']),axis=0)\n",
    "vlens = get_length(v)\n",
    "\n",
    "a = np.concatenate((train_split_noalign['audio'],dev_split_noalign['audio'], test_split_noalign['audio']),axis=0)\n",
    "alens = get_length(a)\n",
    "\n",
    "label = np.concatenate((train_split_noalign['labels'],dev_split_noalign['labels'], test_split_noalign['labels']),axis=0)\n",
    "\n",
    "L_V = v.shape[1]\n",
    "L_A = a.shape[1]\n",
    "\n",
    "\n",
    "all_id = np.concatenate((train_split_noalign['id'], dev_split_noalign['id'], test_split_noalign['id']),axis=0)[:,0]\n",
    "all_id_list = all_id.tolist()\n",
    "\n",
    "train_size = len(train_split_noalign['id'])\n",
    "dev_size = len(dev_split_noalign['id'])\n",
    "test_size = len(test_split_noalign['id'])\n",
    "\n",
    "dev_start = train_size\n",
    "test_start = train_size + dev_size\n",
    "\n",
    "all_csv_id = [(vid[i], str(cid[i])) for i in range(len(vid))]\n",
    "\n",
    "for i, idd in enumerate(all_id_list):\n",
    "    # get the video ID and the features out of the aligned dataset\n",
    "\n",
    "    # matching process\n",
    "    try:\n",
    "        index = i\n",
    "    except:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "\n",
    "    _words = text[index].split()\n",
    "    _label = label[i].astype(np.float32)\n",
    "    _visual = v[i]\n",
    "    _acoustic = a[i]\n",
    "    _vlen = vlens[i]\n",
    "    _alen = alens[i]\n",
    "    _id = '{}[{}]'.format(all_csv_id[0], all_csv_id[1])           \n",
    "\n",
    "    # remove nan values\n",
    "    # label = np.nan_to_num(label)\n",
    "    _visual = np.nan_to_num(_visual)\n",
    "    _acoustic = np.nan_to_num(_acoustic)\n",
    "\n",
    "    # remove speech pause tokens - this is in general helpful\n",
    "    # we should remove speech pauses and corresponding visual/acoustic features together\n",
    "    # otherwise modalities would no longer be aligned\n",
    "    actual_words = []\n",
    "    words = []\n",
    "    visual = []\n",
    "    acoustic = []\n",
    "\n",
    "    for word in _words:\n",
    "        actual_words.append(word)\n",
    "\n",
    "    visual = _visual[L_V - _vlen:,:]\n",
    "    acoustic = _acoustic[L_A - _alen:,:]\n",
    "\n",
    "    if i < dev_start:\n",
    "        train.append((words, visual, acoustic, actual_words, _vlen, _alen, _label, idd))\n",
    "    elif i >= dev_start and i < test_start:\n",
    "        dev.append((words, visual, acoustic, actual_words, _vlen, _alen, _label, idd))\n",
    "    elif i >= test_start:\n",
    "        test.append((words, visual, acoustic, actual_words, _vlen, _alen, _label, idd))\n",
    "    else:\n",
    "        print(f\"Found video that doesn't belong to any splits: {idd}\")\n",
    "\n",
    "\n",
    "# print(f\"Total number of {num_drop} datapoints have been dropped.\")\n",
    "print(f\"Total number of {num_drop} datapoints have been dropped.\")\n",
    "print(\"Dataset split\")\n",
    "print(\"Train Set: {}\".format(len(train)))\n",
    "print(\"Validation Set: {}\".format(len(dev)))\n",
    "print(\"Test Set: {}\".format(len(test)))\n",
    "\n",
    "# Save glove embeddings cache too\n",
    "# self.pretrained_emb = pretrained_emb = load_emb(word2id, config.word_emb_path)\n",
    "# torch.save((pretrained_emb, word2id), CACHE_PATH)\n",
    "pretrained_emb = None\n",
    "\n",
    "# Save pickles\n",
    "to_pickle(train, DATA_PATH + '/dftrain.pkl')\n",
    "to_pickle(dev, DATA_PATH + '/dfdev.pkl')\n",
    "to_pickle(test, DATA_PATH + '/dftest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train,columns=['words', 'visual', 'acoustic', 'actual_words', '_vlen', '_alen', '_label', 'idd'])\n",
    "dev_df = pd.DataFrame(dev,columns=['words', 'visual', 'acoustic', 'actual_words', '_vlen', '_alen', '_label', 'idd'])\n",
    "test_df = pd.DataFrame(test,columns=['words', 'visual', 'acoustic', 'actual_words', '_vlen', '_alen', '_label', 'idd'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPadding(df, colName, dim):\n",
    "    max_length_x = df[colName].apply(lambda x: x.shape[0] if isinstance(x, np.ndarray) else 0).max()\n",
    "\n",
    "    # Perform padding within the DataFrame\n",
    "    df[colName] = df[colName].apply(lambda x: np.vstack([x, np.zeros((max_length_x - x.shape[0], dim))]) if isinstance(x, np.ndarray) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "addPadding(train_df,'visual',35)\n",
    "addPadding(train_df,'acoustic',74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_acoustic_model(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu')(x)  # Add an additional Conv2D layer\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "def create_video_model(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu')(x)  # Add an additional Conv2D layer\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    outputs = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_model(max_length):\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=bert_output)\n",
    "\n",
    "# Modify the create_combined_model function\n",
    "def create_combined_model(acoustic_input_shape, visual_input_shape, text_max_length):\n",
    "    acoustic_model = create_acoustic_model(acoustic_input_shape)\n",
    "    bert_model = create_bert_model(text_max_length)\n",
    "    video_model = create_video_model(visual_input_shape)\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(text_max_length,), dtype=tf.int32)\n",
    "    acoustic_input = tf.keras.layers.Input(shape=acoustic_input_shape)\n",
    "    visual_input = tf.keras.layers.Input(shape=visual_input_shape)\n",
    "\n",
    "    # Get the BERT embeddings sequence (output shape: batch_size, max_length, bert_output_dim)\n",
    "    bert_output = bert_model([input_ids, attention_mask])\n",
    "\n",
    "    # Assuming the output shape of your video_model is (batch_size, video_output_dim)\n",
    "    visual_output = video_model(visual_input)\n",
    "\n",
    "    # Assuming the output shape of your acoustic_model is (batch_size, acoustic_output_dim)\n",
    "    acoustic_output = acoustic_model(acoustic_input)\n",
    "\n",
    "    # Modify the next lines according to your actual requirements for combining the models\n",
    "    # For example, flatten the video_output if needed\n",
    "    visual_output = tf.keras.layers.Flatten()(visual_output)\n",
    "\n",
    "    # Flatten the BERT embeddings sequence to match the visual_output's shape\n",
    "    bert_output = tf.keras.layers.Flatten()(bert_output)\n",
    "\n",
    "    # Concatenate the outputs of the three models\n",
    "    combined_input = tf.keras.layers.Concatenate()([acoustic_output, bert_output, visual_output])\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(combined_input)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[acoustic_input, input_ids, attention_mask, visual_input], outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_df['words'] = train_df['actual_words'].apply(processText)\n",
    "test_df['words'] = test_df['actual_words'].apply(processText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 74)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_df['acoustic'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 35)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_df['visual'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n",
      "maxCacheSize: 24.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 15:33:04.134864: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-28 15:33:04.135055: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 15:33:18.127442: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 15:33:24.539441: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511/511 [==============================] - 2661s 5s/step - loss: 1.2861 - mean_absolute_error: 0.8466\n",
      "Epoch 2/3\n",
      "511/511 [==============================] - 2388s 5s/step - loss: 1.2856 - mean_absolute_error: 0.8465\n",
      "Epoch 3/3\n",
      "511/511 [==============================] - 3702s 7s/step - loss: 1.2856 - mean_absolute_error: 0.8465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1730f4f40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = create_combined_model(acoustic_input_shape=(500,74), visual_input_shape=(500,35), text_max_length=500)\n",
    "\n",
    "# Encode text data using BERT tokenizer\n",
    "def encode_text_data(text_data, max_length=500):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in text_data:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,  # Set max_length to 500 for BERT base model\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "    \n",
    "# ... (Previous code before preparing the data remains unchanged)\n",
    "\n",
    "# Preprocess text data and create a tf.data.Dataset\n",
    "text_data = encode_text_data(train_df['words'].tolist())\n",
    "\n",
    "# Prepare the rest of the data\n",
    "acoustic_data = np.stack(train_df['acoustic'])\n",
    "visual_data = np.stack(train_df['visual'])\n",
    "label_data = np.array([label[0] for label in train_df['_label']])\n",
    "\n",
    "# Unpack the text_data into input_ids_data and attention_mask_data\n",
    "input_ids_data, attention_mask_data = text_data\n",
    "input_ids_data = input_ids_data.numpy().astype(np.int32)\n",
    "attention_mask_data = attention_mask_data.numpy().astype(np.int32)\n",
    "\n",
    "# Convert the label_data to float32 as it is expected for regression\n",
    "label_data = label_data.astype(np.float32)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "model.fit(\n",
    "    [acoustic_data, input_ids_data, attention_mask_data, visual_data],\n",
    "    label_data,\n",
    "    epochs=3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(attention_mask_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16326, 500, 74)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acoustic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16326, 500, 35)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16326, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "addPadding(test_df,'visual',35)\n",
    "addPadding(test_df,'acoustic',74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 20:01:02.250810: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 163s 1s/step\n"
     ]
    }
   ],
   "source": [
    "def encode_text_data(text_data, max_length=500):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in text_data:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,  # Set max_length to 500 for BERT base model\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "    \n",
    "# ... (Previous code before preparing the data remains unchanged)\n",
    "\n",
    "# Preprocess text data and create a tf.data.Dataset\n",
    "text_data = encode_text_data(test_df['words'].tolist())\n",
    "\n",
    "# Prepare the rest of the data\n",
    "acoustic_data = np.stack(test_df['acoustic'])\n",
    "visual_data = np.stack(test_df['visual'])\n",
    "label_data = np.array([label[0] for label in test_df['_label']])\n",
    "\n",
    "# Unpack the text_data into input_ids_data and attention_mask_data\n",
    "input_ids_data, attention_mask_data = text_data\n",
    "input_ids_data = input_ids_data.numpy().astype(np.int32)\n",
    "attention_mask_data = attention_mask_data.numpy().astype(np.int32)\n",
    "\n",
    "# Convert the label_data to float32 as it is expected for regression\n",
    "label_data = label_data.astype(np.float32)\n",
    "predictions = model.predict([acoustic_data, input_ids_data, attention_mask_data, visual_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.2492965\n",
      "Mean Absolute Error: 0.83841306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(label_data, predictions)\n",
    "mae = mean_absolute_error(label_data, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn while saving (showing 5 of 424). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./savedModels/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./savedModels/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the model to a directory\n",
    "model.save('./savedModels')\n",
    "\n",
    "# Optionally, you can save the tokenizer used for encoding the text data\n",
    "# tokenizer.save_pretrained('./savedModels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('env_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7f7a6c7a328c331394eefecd3ffebe18a341a9e21c7b3790bdb1b050ca99952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
